{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_augmentation.ipynb","provenance":[{"file_id":"12-mOSUZuEsgeAkOFAWXM_IiHrxITLhks","timestamp":1589735473582}],"collapsed_sections":["WTBUvYrKj5Q2","TuVM5YbCu98K"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2ec919c1cb0c49a0a45c191921bd2fa0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_91f832f09b3d47f4a6363709c334a3af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c249dab4acdd4403b0bc04193930d34a","IPY_MODEL_99db823750d54931966bc47d0cf86460"]}},"91f832f09b3d47f4a6363709c334a3af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c249dab4acdd4403b0bc04193930d34a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_57e81f877d4745ea981dbe5aceddbc23","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9de40259c7649d0bae5e54f423c5ee3"}},"99db823750d54931966bc47d0cf86460":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_09d9b68dde52445ba4d54d8e57dbf2a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 175MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1f14bdb40ce14c6e8047a8c568187ab7"}},"57e81f877d4745ea981dbe5aceddbc23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c9de40259c7649d0bae5e54f423c5ee3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09d9b68dde52445ba4d54d8e57dbf2a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1f14bdb40ce14c6e8047a8c568187ab7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1rmI2rnr5e48","colab_type":"text"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"H586gtoqJ0cu","colab_type":"text"},"source":["## Imports "]},{"cell_type":"code","metadata":{"id":"_2X1OcpKmO35","colab_type":"code","outputId":"21360e9c-5f96-4410-8acc-561e21330aa5","executionInfo":{"status":"ok","timestamp":1592211841912,"user_tz":-120,"elapsed":37939,"user":{"displayName":"Matteo Bunino","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwZwD9gAYbPOfNJoxSUyCvD_aRYvQTPmt2RxWUZg=s64","userId":"14931582740568623850"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Hu65BTA5ikD","colab_type":"code","colab":{}},"source":["from PIL import Image\n","from sklearn import utils\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.model_selection import train_test_split\n","from torch.autograd import Function\n","from torch.backends import cudnn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Subset, DataLoader\n","from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","from torchvision import models\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.datasets import VisionDataset\n","from torchvision.models import alexnet\n","from torchvision.transforms.functional import pad\n","from tqdm import tqdm\n","import logging\n","import matplotlib.pyplot as plt\n","import numbers\n","import numpy as np\n","import os\n","import os.path\n","import shutil\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import zipfile\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTBUvYrKj5Q2","colab_type":"text"},"source":["## Download datasets in colab\n","Always working for everyone"]},{"cell_type":"code","metadata":{"id":"lvMQqD8Wj4Vi","colab_type":"code","outputId":"0a54ba3a-01d1-465c-c5a9-2cd38ccb7f94","executionInfo":{"status":"ok","timestamp":1592211902881,"user_tz":-120,"elapsed":56481,"user":{"displayName":"Matteo Bunino","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwZwD9gAYbPOfNJoxSUyCvD_aRYvQTPmt2RxWUZg=s64","userId":"14931582740568623850"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["# Download ROD dataset\n","rod_destination_path = \"/content/ROD\"\n","if not os.path.isdir(rod_destination_path):\n","  # ROD \n","  # https://drive.google.com/open?id=1p1GORdB44NjtNWJ4d1xqttseM1X9lWNF\n","  # https://drive.google.com/open?id=168neCvaHwMffFOqjOkth-wVaP4tRFuSW\n","  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1p1GORdB44NjtNWJ4d1xqttseM1X9lWNF\" > /dev/null\n","  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1p1GORdB44NjtNWJ4d1xqttseM1X9lWNF\" -o \"ROD.zip\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   4340      0 --:--:-- --:--:-- --:--:--  4340\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 3146M    0 3146M    0     0  59.9M      0 --:--:--  0:00:52 --:--:-- 23.4M\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WKRdgvgGkJ4f","colab_type":"code","outputId":"32a189a5-dcb1-443a-e5c3-0cab03bf537f","executionInfo":{"status":"ok","timestamp":1592212038690,"user_tz":-120,"elapsed":192281,"user":{"displayName":"Matteo Bunino","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwZwD9gAYbPOfNJoxSUyCvD_aRYvQTPmt2RxWUZg=s64","userId":"14931582740568623850"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["# Download synROD dataset\n","synrod_destination_path = \"/content/synROD\"\n","if not os.path.isdir(synrod_destination_path):\n","  # synROD \n","  # https://drive.google.com/open?id=1rry4GViJLmmMpbm0B2s7MyQs5Dx8pFS3\n","  # https://drive.google.com/open?id=1V1fthSNAvsPRF6hLt_kf_xonw7lxAV03\n","  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1rry4GViJLmmMpbm0B2s7MyQs5Dx8pFS3\" > /dev/null\n","  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1rry4GViJLmmMpbm0B2s7MyQs5Dx8pFS3\" -o \"synROD.zip\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   5230      0 --:--:-- --:--:-- --:--:--  5230\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 5343M    0 5343M    0     0  40.2M      0 --:--:--  0:02:12 --:--:-- 74.0M\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V3f6vZnCkQuk","colab_type":"code","outputId":"e6629a66-cbe3-4476-8be2-4d2891d62b1b","executionInfo":{"status":"ok","timestamp":1592212325232,"user_tz":-120,"elapsed":478492,"user":{"displayName":"Matteo Bunino","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwZwD9gAYbPOfNJoxSUyCvD_aRYvQTPmt2RxWUZg=s64","userId":"14931582740568623850"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Extract ROD dataset\n","rod_destination_path = \"/content/ROD\"\n","if not os.path.isdir(rod_destination_path):\n","  with zipfile.ZipFile(\"/content/ROD.zip\", 'r') as zip_ref:\n","      zip_ref.extractall(rod_destination_path)\n","!rm ROD.zip\n","\n","# Extract synROD dataset\n","synrod_destination_path = \"/content/synROD\"\n","if not os.path.isdir(synrod_destination_path):\n","  with zipfile.ZipFile(\"/content/synROD.zip\", 'r') as zip_ref:\n","      zip_ref.extractall(synrod_destination_path)\n","!rm synROD.zip\n","!rm cookie\n","\n","rod_path = \"/content/ROD/ROD\"\n","synrod_path = \"/content/synROD/synROD\"\n","\n","\"\"\"\n","if os.path.isdir(os.path.join(synrod_path,  \"bell_papper\")):\n","  # line below needed only for the first extraction of the synrod dataset, in which bell_pepper is wrongly named bell_papper\n","  os.rename(os.path.join(synrod_path,  \"bell_papper\"), os.path.join(synrod_path,  \"bell_pepper\") )\n","  print(\"Bell pepper fixed\")\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nif os.path.isdir(os.path.join(synrod_path,  \"bell_papper\")):\\n  # line below needed only for the first extraction of the synrod dataset, in which bell_pepper is wrongly named bell_papper\\n  os.rename(os.path.join(synrod_path,  \"bell_papper\"), os.path.join(synrod_path,  \"bell_pepper\") )\\n  print(\"Bell pepper fixed\")\\n'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"eWXTNv9OJsnr","colab_type":"text"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"EF4lKNlJJvAy","colab_type":"code","colab":{}},"source":["\n","# DATALOADER function\n","def collate(batch):\n","  return batch\n","\n","# helper function used to setup batches returned by the dataloaders in the way that is mentioned in the paper\n","def format_batch(batch, pretext_task=\"rotation\"):\n","  \"\"\"\"\n","  set pretext_task == rotation or zoom_clf, to require the pretext task labels to be of type \"long\"\n","  set pretext_task == zoom to require the pretext task labels to be of type \"float\"\n","  \"\"\"\n","  data = {\"rgb\":[], \"depth\":[], \"label\":[] }\n","  data_hat = {\"rgb\":[], \"depth\":[], \"label\":[] }\n","  for tuple_, tuple_hat in batch:\n","    rgb_img, depth_img, label = tuple_\n","    rot_rgb_img, rot_depth_img, rot_label = tuple_hat\n","\n","    data[\"rgb\"].append(rgb_img[None,:])\n","    data[\"depth\"].append(depth_img[None,:])\n","    data[\"label\"].append(label)\n","\n","    data_hat[\"rgb\"].append(rot_rgb_img[None,:])\n","    data_hat[\"depth\"].append(rot_depth_img[None,:])\n","    data_hat[\"label\"].append(rot_label)\n","  \n","  data[\"rgb\"] = torch.cat(data[\"rgb\"] , dim=0) \n","  data[\"depth\"] = torch.cat(data[\"depth\"] , dim=0)\n","  data[\"label\"] = torch.LongTensor(data[\"label\"])\n","  \n","  data_hat[\"rgb\"] = torch.cat(data_hat[\"rgb\"] , dim=0) \n","  data_hat[\"depth\"] = torch.cat(data_hat[\"depth\"] , dim=0)\n","  if pretext_task == \"rotation\" or pretext_task == \"zoom_clf\":\n","    data_hat[\"label\"] = torch.LongTensor(data_hat[\"label\"] )\n","  else:\n","    data_hat[\"label\"] = torch.FloatTensor(data_hat[\"label\"] )\n","  \n","  return data, data_hat"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9O8uN5WxTOEm","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def learning_curves(training_accuracies, training_losses, validation_accuracies, validation_losses, plot_title, plot_size=(16,6)):\n","  \"\"\"\n","  Plots accuracies and losses per epochs.\n","  \"\"\"\n","  fig, ax = plt.subplots(nrows=1, ncols=2, figsize=plot_size)\n","  ax[0].plot(range(1,len(training_accuracies)+1), training_accuracies, label=\"Training\")\n","  ax[0].plot(range(1,len(validation_accuracies)+1), validation_accuracies, label=\"Validation\")\n","  ax[0].legend()\n","  ax[0].set_title(\"Accuracy\")\n","  ax[0].set_xlabel(\"Epochs\")\n","\n","  ax[1].plot(range(1,len(training_losses)+1), training_losses, label=\"Training\")\n","  ax[1].plot(range(1,len(validation_losses)+1), validation_losses, label=\"Validation\")\n","  ax[1].legend()\n","  ax[1].set_title(\"Loss\")\n","  ax[1].set_xlabel(\"Epochs\")\n","\n","  fig.suptitle(plot_title)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NrdoKOpuxud","colab_type":"code","colab":{}},"source":["# Entropy loss\n","class HLoss(nn.Module):\n","    def __init__(self):\n","        super(HLoss, self).__init__()\n","\n","    def forward(self, x):\n","        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n","        b = -1.0 * b.sum()\n","        return b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D1wk53faKD3A","colab_type":"text"},"source":["## Path variables declaration"]},{"cell_type":"code","metadata":{"id":"6GcE2lPAKDZy","colab_type":"code","colab":{}},"source":["rod_path = \"/content/ROD/ROD\"\n","synrod_path = \"/content/synROD/synROD\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRk1caT8K31v","colab_type":"text"},"source":["## Copy in current folder datasets and net classes"]},{"cell_type":"code","metadata":{"id":"4B__Yny4K9o3","colab_type":"code","colab":{}},"source":["!cp -r \"/content/drive/My Drive/DL_project/architecture/dataset/.\" \"/content/\"\n","!cp -r \"/content/drive/My Drive/DL_project/architecture/net/.\" \"/content/\"\n","!cp -r \"/content/drive/My Drive/DL_project/architecture/transform_config/.\" \"/content/\"\n","!cp -r \"/content/drive/My Drive/DL_project/architecture/datasets_with_splits/.\" \"/content/\"\n","!cp -r \"/content/drive/My Drive/DL_project/data_split/.\" \"/content/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sp9oWROBKxqm","colab_type":"text"},"source":["## Import datasets, net and configurator classes"]},{"cell_type":"code","metadata":{"id":"IEaktSSiK2L2","colab_type":"code","colab":{}},"source":["from synrod import SynROD\n","from rod import ROD\n","\n","from synrodmod import SynRODMOD\n","from rodmod import RODMOD\n","\n","from dnet2 import DNet\n","from tconfig import TransformConfig"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivOnu9RQ2Miz","colab_type":"text"},"source":["# Train test function definitions"]},{"cell_type":"markdown","metadata":{"id":"hArWi68zqQcr","colab_type":"text"},"source":["## Evaluate net"]},{"cell_type":"code","metadata":{"id":"EIMxYuhuqQkG","colab_type":"code","colab":{}},"source":["def evaluate_net(net, eval_dataloader, pretext_task):\n","  net.eval()\n","  validation_corrects = 0\n","  validation_main_loss = 0\n","  validation_dim = 0\n","  n_iters = 0\n","  \n","  for source_val_batch in eval_dataloader:\n","    S, _ = format_batch(source_val_batch, pretext_task=pretext_task)\n","  \n","    # MAIN TASK\n","    # setup SOURCE DOMAIN STANDARD dataset to feed to the net\n","    source_rgb_images = S[\"rgb\"].to(DEVICE)\n","    source_depth_images = S[\"depth\"].to(DEVICE)\n","    source_main_labels = S[\"label\"].to(DEVICE)\n","    \n","    outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\")\n","    loss_M = criterion(outputs, source_main_labels)\n","    \n","    _, preds = torch.max(outputs.data, 1)\n","    validation_corrects += torch.sum(preds == source_main_labels.data).data.item()\n","    validation_dim += len(source_main_labels)\n","    validation_main_loss += loss_M.item()\n","\n","    del source_rgb_images, source_depth_images, source_main_labels\n","\n","    n_iters += 1\n","\n","    return validation_corrects / validation_dim, validation_main_loss / n_iters\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1uvqfWHZQ2mH","colab_type":"text"},"source":["## Relative rotation"]},{"cell_type":"code","metadata":{"id":"P57r29_XQ5M9","colab_type":"code","colab":{}},"source":["def train_test_ours(synrod, synrod_validation, rod, rod_test, hyperparams, save_folder, net_name=\"ours_net\", light_validation=False, preexisting_net=None):\n","  \"\"\"Train the architecture called \"OURS\" in the reference paper with rotation pretext task.\n","  The net is trained in end-to-end fashion.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        lambda (!!!)\n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","  lambda_ = hyperparams[\"lambda\"]\n","  em_weight = 0.1\n","  \n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  NUM_WORKERS = 4\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  source_rot = DataLoader(synrod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target_rot = DataLoader(rod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","\n","  # evaluation dataloaders (no data augmentation on these)\n","  source_validation = DataLoader(synrod_validation,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target_test = DataLoader(rod_test,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","\n","  # smallest dataloader dim\n","  small_dl_dim = min([len(source), len(source_rot), len(target_rot), len(target)])\n","\n","  # datasets dimensions\n","  source_dim = len(synrod)\n","  target_dim = len(rod)\n","  validation_dim = len(synrod_validation)\n","  test_dim = len(rod_test)\n","\n","  # NET DEFINITION\n","  net = DNet(num_classes=47, dim_pretext=4).to(DEVICE) if preexisting_net is None else preexisting_net.to(DEVICE)\n","\n","  criterion = nn.CrossEntropyLoss() \n","  entropy_min_criterion = HLoss()\n","\n","  parameters_to_optimize = net.parameters() \n","  optimizer = optim.SGD(parameters_to_optimize, lr=lr, \n","                            momentum=curr_momentum, \n","                            weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  # lists that accumulate loss/accuracy values over the training period\n","  train_loss = []\n","  train_acc = []\n","  valid_loss = []\n","  valid_acc = []\n","  test_loss = []\n","  test_acc = []\n"," \n","  \n","  for i in range(epochs):\n","    train_main_corrects = 0\n","    train_main_loss = 0\n","\n","    train_rot_source_corrects = 0\n","    train_rot_source_loss = 0\n","\n","    train_rot_target_corrects = 0\n","    train_rot_target_loss = 0\n","\n","    n_iters = 0\n","\n","    print(\"EPOCH: \", i + 1)\n","\n","    for source_batch, target_batch, source_rot_batch, target_rot_batch in tqdm(zip(source, target, source_rot, target_rot), total=small_dl_dim):\n","      \n","      net.train()\n","      S, _ = format_batch(source_batch)\n","      T, _ = format_batch(target_batch)\n","\n","      _, S_hat = format_batch(source_rot_batch, pretext_task=\"rotation\")\n","      _, T_hat = format_batch(target_rot_batch, pretext_task=\"rotation\")\n","\n","      # zero the gradients\n","      optimizer.zero_grad() \n","\n","      # ------------------------- MAIN TASK --------------------------\n","      # setup SOURCE DOMAIN STANDARD dataset to feed to the net\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      # train on source original images \n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\")\n","      loss_M = criterion(outputs, source_main_labels)\n","      loss_M.backward()\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      train_main_loss += loss_M.item()\n","      train_main_corrects += running_corrects\n","\n","      tot_samples = len(source_main_labels)\n","\n","      del source_rgb_images, source_depth_images, source_main_labels\n","\n","      # entropy minimization\n","      # setup TARGET DOMAIN STANDARD dataset to feed to the net\n","      target_rgb_images = T[\"rgb\"].to(DEVICE)\n","      target_depth_images = T[\"depth\"].to(DEVICE)\n","      # target labels in this phase can't be used for training\n","      # train on source original images \n","      outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","      loss_entropy_min = entropy_min_criterion(outputs)\n","      loss_ent = (em_weight/tot_samples)*loss_entropy_min\n","      loss_ent.backward()\n","\n","      del target_rgb_images, target_depth_images\n","\n","\n","      # ------------------------- PRETEXT TASK -------------------------------\n","      # setup  SOURCE DOMAIN ROTATED dataset to feed to the net\n","      source_rotated_rgb_images = S_hat[\"rgb\"].to(DEVICE)\n","      source_rotated_depth_images = S_hat[\"depth\"].to(DEVICE)\n","      source_rotated_labels = S_hat[\"label\"].to(DEVICE)\n","      \n","      # train on source rotated \n","      outputs = net.forward(source_rotated_rgb_images, source_rotated_depth_images, mode=\"pretext\")\n","      loss_P_1 = criterion(outputs, source_rotated_labels) \n","      lossP1 = lambda_*loss_P_1\n","      lossP1.backward()\n","\n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_rotated_labels.data).data.item()\n","      train_rot_source_corrects += running_corrects\n","      train_rot_source_loss += loss_P_1.item()\n","\n","      del source_rotated_rgb_images, source_rotated_depth_images, source_rotated_labels\n","\n","\n","      #setup TARGET DOMAIN ROTATED dataset to feed to the net\n","      target_rotated_rgb_images = T_hat[\"rgb\"].to(DEVICE)\n","      target_rotated_depth_images = T_hat[\"depth\"].to(DEVICE)\n","      target_rotated_labels = T_hat[\"label\"].to(DEVICE)\n","      \n","      # train on target rotated\n","      outputs = net.forward(target_rotated_rgb_images, target_rotated_depth_images, mode=\"pretext\")\n","      loss_P_2 = criterion(outputs, target_rotated_labels)\n","      lossP2 = lambda_*loss_P_2\n","      lossP2.backward()\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == target_rotated_labels.data).data.item()\n","      train_rot_target_corrects += running_corrects\n","      train_rot_target_loss += loss_P_2.item()\n","\n","      del target_rotated_rgb_images, target_rotated_depth_images, target_rotated_labels\n","\n","    \n","      # UPDATE WEIGHTS\n","      optimizer.step()\n","\n","      n_iters += 1\n","\n","    \n","    train_loss.append(train_main_loss / n_iters)\n","    train_acc.append(train_main_corrects / source_dim)\n","    \n","    print(\"train main accuracy: \", train_main_corrects / source_dim)\n","    print(\"train main loss: \", train_main_loss / n_iters)\n","    print(\"train rot source accuracy: \", train_rot_source_corrects / source_dim)\n","    print(\"train rot source loss: \", train_rot_source_loss / n_iters)\n","    print(\"train rot target accuracy: \", train_rot_target_corrects / target_dim)\n","    print(\"train rot target loss: \", train_rot_target_loss / n_iters)\n","\n","    \n","    # ---------------------- EVALUATION -----------------------------------\n","    if not light_validation or i == epochs - 1:\n","      # VALIDATION ON SOURCE\n","      val_acc, val_loss = evaluate_net(net, source_validation, \"rotation\")\n","\n","      valid_loss.append(val_loss)\n","      valid_acc.append(val_acc)\n","      print(\"validation main accuracy: \", val_acc)\n","      print(\"validation main loss: \", val_loss)\n","\n","      \n","    if i == epochs - 1 or (i + 5) % 1 == 0:\n","      # TEST ON TARGET\n","      tst_acc, tst_loss = evaluate_net(net, target_test, \"rotation\")\n","  \n","      test_loss.append(tst_losss)\n","      test_acc.append(tst_acc)\n","      print(\"test main target accuracy: \", tst_acc)\n","      print(\"test main target loss: \", tst_loss)\n","\n","\n","    print()\n","    scheduler.step()\n","\n","    # SAVE NET: every 5 epochs and at the last one\n","    if (i + 1) % 5 == 0 or i == epochs - 1:\n","      model_save_path = os.path.join(save_folder,  net_name + \".pth\"  )\n","      torch.save(net.state_dict(), model_save_path)\n","  \n","  return net, train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TuVM5YbCu98K","colab_type":"text"},"source":["## Relative zoom (classification)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cJnTWIAsyci7","colab":{}},"source":["def train_test_zoom_clf(synrod, synrod_validation, rod, rod_test, hyperparams, save_folder, net_name=\"ours_net\", light_validation=False, preexisting_net=None):\n","  \"\"\"Our pretext task variation, implemented as a classification problem to predict the relative zoom between rgb and depth.\n","  The net is trained in end-to-end fashion.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        lambda (!!!)\n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","  lambda_ = hyperparams[\"lambda\"]\n","  em_weight = 0.1\n","  \n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  NUM_WORKERS = 4\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  source_rot = DataLoader(synrod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target_rot = DataLoader(rod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","\n","  # evaluation dataloaders (no data augmentation on these)\n","  source_validation = DataLoader(synrod_validation,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target_test = DataLoader(rod_test,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","\n","  # smallest dataloader dim\n","  small_dl_dim = min([len(source), len(source_rot), len(target_rot), len(target)])\n","\n","  # datasets dimensions\n","  source_dim = len(synrod)\n","  target_dim = len(rod)\n","  validation_dim = len(synrod_validation)\n","  test_dim = len(rod_test)\n","\n","  # NET DEFINITION\n","  net = DNet(num_classes=47, dim_pretext=5).to(DEVICE) if preexisting_net is None else preexisting_net.to(DEVICE)\n","\n","  classification_criterion = nn.CrossEntropyLoss() \n","  entropy_min_criterion = HLoss()\n","\n","  parameters_to_optimize = net.parameters() \n","  optimizer = optim.SGD(parameters_to_optimize, lr=lr, \n","                            momentum=curr_momentum, \n","                            weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  # lists that accumulate loss/accuracy values over the training period\n","  train_loss = []\n","  train_acc = []\n","  valid_loss = []\n","  valid_acc = []\n","  test_loss = []\n","  test_acc = []\n"," \n","  \n","  for i in range(epochs):\n","    train_main_corrects = 0\n","    train_main_loss = 0\n","\n","    train_rot_source_corrects = 0\n","    train_rot_source_loss = 0\n","\n","    train_rot_target_corrects = 0\n","    train_rot_target_loss = 0\n","\n","    n_iters = 0\n","\n","    print(\"EPOCH: \", i + 1)\n","\n","    for source_batch, target_batch, source_rot_batch, target_rot_batch in tqdm(zip(source, target, source_rot, target_rot), total=small_dl_dim):\n","      \n","      net.train()\n","      S, _ = format_batch(source_batch)\n","      T, _ = format_batch(target_batch)\n","\n","      _, S_hat = format_batch(source_rot_batch, pretext_task=\"zoom_clf\")\n","      _, T_hat = format_batch(target_rot_batch, pretext_task=\"zoom_clf\")\n","\n","      # zero the gradients\n","      optimizer.zero_grad() \n","\n","      # ------------------------- MAIN TASK --------------------------\n","      # setup SOURCE DOMAIN STANDARD dataset to feed to the net\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      # train on source original images \n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\")\n","      loss_M = classification_criterion(outputs, source_main_labels)\n","      loss_M.backward()\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      train_main_loss += loss_M.item()\n","      train_main_corrects += running_corrects\n","\n","      tot_samples = len(source_main_labels)\n","\n","      del source_rgb_images, source_depth_images, source_main_labels\n","\n","      # entropy minimization\n","      # setup TARGET DOMAIN STANDARD dataset to feed to the net\n","      target_rgb_images = T[\"rgb\"].to(DEVICE)\n","      target_depth_images = T[\"depth\"].to(DEVICE)\n","      # target labels in this phase can't be used for training\n","      # train on source original images \n","      outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","      loss_entropy_min = entropy_min_criterion(outputs)\n","      loss_ent = (em_weight/tot_samples)*loss_entropy_min\n","      loss_ent.backward()\n","\n","      del target_rgb_images, target_depth_images\n","\n","\n","      # ------------------------- PRETEXT TASK -------------------------------\n","      # setup  SOURCE DOMAIN ROTATED dataset to feed to the net\n","      source_rotated_rgb_images = S_hat[\"rgb\"].to(DEVICE)\n","      source_rotated_depth_images = S_hat[\"depth\"].to(DEVICE)\n","      source_rotated_labels = S_hat[\"label\"].to(DEVICE)\n","      \n","      # train on source rotated \n","      outputs = net.forward(source_rotated_rgb_images, source_rotated_depth_images, mode=\"pretext\")\n","      loss_P_1 = classification_criterion(outputs, source_rotated_labels) \n","      lossP1 = lambda_*loss_P_1\n","      lossP1.backward()\n","\n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_rotated_labels.data).data.item()\n","      train_rot_source_corrects += running_corrects\n","      train_rot_source_loss += loss_P_1.item()\n","\n","      del source_rotated_rgb_images, source_rotated_depth_images, source_rotated_labels\n","\n","\n","      #setup TARGET DOMAIN ROTATED dataset to feed to the net\n","      target_rotated_rgb_images = T_hat[\"rgb\"].to(DEVICE)\n","      target_rotated_depth_images = T_hat[\"depth\"].to(DEVICE)\n","      target_rotated_labels = T_hat[\"label\"].to(DEVICE)\n","      \n","      # train on target rotated\n","      outputs = net.forward(target_rotated_rgb_images, target_rotated_depth_images, mode=\"pretext\")\n","      loss_P_2 = classification_criterion(outputs, target_rotated_labels)\n","      lossP2 = lambda_*loss_P_2\n","      lossP2.backward()\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == target_rotated_labels.data).data.item()\n","      train_rot_target_corrects += running_corrects\n","      train_rot_target_loss += loss_P_2.item()\n","\n","      del target_rotated_rgb_images, target_rotated_depth_images, target_rotated_labels\n","\n","    \n","      # UPDATE WEIGHTS\n","      optimizer.step()\n","\n","      n_iters += 1\n","\n","    \n","    train_loss.append(train_main_loss / n_iters)\n","    train_acc.append(train_main_corrects / source_dim)\n","    \n","    print(\"train main accuracy: \", train_main_corrects / source_dim)\n","    print(\"train main loss: \", train_main_loss / n_iters)\n","    print(\"train rot source accuracy: \", train_rot_source_corrects / source_dim)\n","    print(\"train rot source loss: \", train_rot_source_loss / n_iters)\n","    print(\"train rot target accuracy: \", train_rot_target_corrects / target_dim)\n","    print(\"train rot target loss: \", train_rot_target_loss / n_iters)\n","\n","    \n","    # ---------------------- EVALUATION -----------------------------------\n","    if not light_validation or i == epochs - 1:\n","      # VALIDATION ON SOURCE\n","      val_acc, val_loss = evaluate_net(net, source_validation, \"zoom_clf\")\n","\n","      valid_loss.append(val_loss)\n","      valid_acc.append(val_acc)\n","      print(\"validation main accuracy: \", val_acc)\n","      print(\"validation main loss: \", val_loss)\n","\n","      \n","    if i == epochs - 1 or (i + 5) % 1 == 0:\n","      # TEST ON TARGET\n","      tst_acc, tst_loss = evaluate_net(net, target_test, \"zoom_clf\")\n","  \n","      test_loss.append(tst_losss)\n","      test_acc.append(tst_acc)\n","      print(\"test main target accuracy: \", tst_acc)\n","      print(\"test main target loss: \", tst_loss)\n","\n","\n","    print()\n","    scheduler.step()\n","\n","    # SAVE NET: every 5 epochs and at the last one\n","    if (i + 1) % 5 == 0 or i == epochs - 1:\n","      model_save_path = os.path.join(save_folder,  net_name + \".pth\"  )\n","      torch.save(net.state_dict(), model_save_path)\n","  \n","  return net, train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vfC0V5pM06J_"},"source":["## Relative zoom (regression)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GlhMZOS_06KF","colab":{}},"source":["def train_test_zoom_clf(synrod, synrod_validation, rod, rod_test, hyperparams, save_folder, net_name=\"ours_net\", light_validation=False, preexisting_net=None):\n","  \"\"\"Our pretext task variation, implemented as a regression problem to predict the relative zoom between rgb and depth.\n","  The net is trained in end-to-end fashion.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        lambda (!!!)\n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","  lambda_ = hyperparams[\"lambda\"]\n","  em_weight = 0.1\n","  \n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  NUM_WORKERS = 4\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  source_rot = DataLoader(synrod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target_rot = DataLoader(rod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","\n","  # evaluation dataloaders (no data augmentation on these)\n","  source_validation = DataLoader(synrod_validation,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","  target_test = DataLoader(rod_test,  batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)\n","\n","  # smallest dataloader dim\n","  small_dl_dim = min([len(source), len(source_rot), len(target_rot), len(target)])\n","\n","  # datasets dimensions\n","  source_dim = len(synrod)\n","  target_dim = len(rod)\n","  validation_dim = len(synrod_validation)\n","  test_dim = len(rod_test)\n","\n","  # NET DEFINITION\n","  net = DNet(num_classes=47, dim_pretext=1).to(DEVICE) if preexisting_net is None else preexisting_net.to(DEVICE)\n","\n","  classification_criterion = nn.CrossEntropyLoss() \n","  regression_criterion = nn.MSELoss()\n","  entropy_min_criterion = HLoss()\n","\n","  parameters_to_optimize = net.parameters() \n","  optimizer = optim.SGD(parameters_to_optimize, lr=lr, \n","                            momentum=curr_momentum, \n","                            weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  # lists that accumulate loss/accuracy values over the training period\n","  train_loss = []\n","  train_acc = []\n","  valid_loss = []\n","  valid_acc = []\n","  test_loss = []\n","  test_acc = []\n"," \n","  \n","  for i in range(epochs):\n","    train_main_corrects = 0\n","    train_main_loss = 0\n","\n","    train_rot_source_corrects = 0\n","    train_rot_source_loss = 0\n","\n","    train_rot_target_corrects = 0\n","    train_rot_target_loss = 0\n","\n","    n_iters = 0\n","\n","    print(\"EPOCH: \", i + 1)\n","\n","    for source_batch, target_batch, source_rot_batch, target_rot_batch in tqdm(zip(source, target, source_rot, target_rot), total=small_dl_dim):\n","      \n","      net.train()\n","      S, _ = format_batch(source_batch)\n","      T, _ = format_batch(target_batch)\n","\n","      _, S_hat = format_batch(source_rot_batch, pretext_task=\"zoom_reg\")\n","      _, T_hat = format_batch(target_rot_batch, pretext_task=\"zoom_reg\")\n","\n","      # zero the gradients\n","      optimizer.zero_grad() \n","\n","      # ------------------------- MAIN TASK --------------------------\n","      # setup SOURCE DOMAIN STANDARD dataset to feed to the net\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      # train on source original images \n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\")\n","      loss_M = classification_criterion(outputs, source_main_labels)\n","      loss_M.backward()\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      train_main_loss += loss_M.item()\n","      train_main_corrects += running_corrects\n","\n","      tot_samples = len(source_main_labels)\n","\n","      del source_rgb_images, source_depth_images, source_main_labels\n","\n","      # entropy minimization\n","      # setup TARGET DOMAIN STANDARD dataset to feed to the net\n","      target_rgb_images = T[\"rgb\"].to(DEVICE)\n","      target_depth_images = T[\"depth\"].to(DEVICE)\n","      # target labels in this phase can't be used for training\n","      # train on source original images \n","      outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","      loss_entropy_min = entropy_min_criterion(outputs)\n","      loss_ent = (em_weight/tot_samples)*loss_entropy_min\n","      loss_ent.backward()\n","\n","      del target_rgb_images, target_depth_images\n","\n","\n","      # ------------------------- PRETEXT TASK -------------------------------\n","      # setup  SOURCE DOMAIN ROTATED dataset to feed to the net\n","      source_rotated_rgb_images = S_hat[\"rgb\"].to(DEVICE)\n","      source_rotated_depth_images = S_hat[\"depth\"].to(DEVICE)\n","      source_rotated_labels = S_hat[\"label\"].to(DEVICE)\n","      \n","      # train on source rotated \n","      outputs = net.forward(source_rotated_rgb_images, source_rotated_depth_images, mode=\"pretext\")\n","      loss_P_1 = regression_criterion(outputs.flatten(), source_rotated_labels) \n","      lossP1 = lambda_*loss_P_1\n","      lossP1.backward()\n","\n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_rotated_labels.data).data.item()\n","      train_rot_source_corrects += running_corrects\n","      train_rot_source_loss += loss_P_1.item()\n","\n","      del source_rotated_rgb_images, source_rotated_depth_images, source_rotated_labels\n","\n","\n","      #setup TARGET DOMAIN ROTATED dataset to feed to the net\n","      target_rotated_rgb_images = T_hat[\"rgb\"].to(DEVICE)\n","      target_rotated_depth_images = T_hat[\"depth\"].to(DEVICE)\n","      target_rotated_labels = T_hat[\"label\"].to(DEVICE)\n","      \n","      # train on target rotated\n","      outputs = net.forward(target_rotated_rgb_images, target_rotated_depth_images, mode=\"pretext\")\n","      loss_P_2 = regression_criterion(outputs.flatten(), target_rotated_labels)\n","      lossP2 = lambda_*loss_P_2\n","      lossP2.backward()\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == target_rotated_labels.data).data.item()\n","      train_rot_target_corrects += running_corrects\n","      train_rot_target_loss += loss_P_2.item()\n","\n","      del target_rotated_rgb_images, target_rotated_depth_images, target_rotated_labels\n","\n","    \n","      # UPDATE WEIGHTS\n","      optimizer.step()\n","\n","      n_iters += 1\n","\n","    \n","    train_loss.append(train_main_loss / n_iters)\n","    train_acc.append(train_main_corrects / source_dim)\n","    \n","    print(\"train main accuracy: \", train_main_corrects / source_dim)\n","    print(\"train main loss: \", train_main_loss / n_iters)\n","    print(\"train rot source accuracy: \", train_rot_source_corrects / source_dim)\n","    print(\"train rot source loss: \", train_rot_source_loss / n_iters)\n","    print(\"train rot target accuracy: \", train_rot_target_corrects / target_dim)\n","    print(\"train rot target loss: \", train_rot_target_loss / n_iters)\n","\n","    \n","    # ---------------------- EVALUATION -----------------------------------\n","    if not light_validation or i == epochs - 1:\n","      # VALIDATION ON SOURCE\n","      val_acc, val_loss = evaluate_net(net, source_validation, \"zoom_reg\")\n","\n","      valid_loss.append(val_loss)\n","      valid_acc.append(val_acc)\n","      print(\"validation main accuracy: \", val_acc)\n","      print(\"validation main loss: \", val_loss)\n","\n","      \n","    if i == epochs - 1 or (i + 5) % 1 == 0:\n","      # TEST ON TARGET\n","      tst_acc, tst_loss = evaluate_net(net, target_test, \"zoom_reg\")\n","  \n","      test_loss.append(tst_losss)\n","      test_acc.append(tst_acc)\n","      print(\"test main target accuracy: \", tst_acc)\n","      print(\"test main target loss: \", tst_loss)\n","\n","\n","    print()\n","    scheduler.step()\n","\n","    # SAVE NET: every 5 epochs and at the last one\n","    if (i + 1) % 5 == 0 or i == epochs - 1:\n","      model_save_path = os.path.join(save_folder,  net_name + \".pth\"  )\n","      torch.save(net.state_dict(), model_save_path)\n","  \n","  return net, train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnQWVkBVR5lo","colab_type":"text"},"source":["# Experiment section"]},{"cell_type":"markdown","metadata":{"id":"lqu-MLxRSdc2","colab_type":"text"},"source":["### define normalization transformations"]},{"cell_type":"code","metadata":{"id":"RYuWC0qOyxfd","colab_type":"code","colab":{}},"source":["tfConfig = TransformConfig(resize_shape=256, centercrop_shape=224)   # or resize_shape=224                              # config types are imagenet, rgb_mod, depth_mod, rgb_depth_mod\n","synrod_param_values, rod_param_values = tfConfig.get_rotation_configuration(config_type=\"imagenet\")    # mod corresponds to the modification of imagenet weights with the computed ones"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tmt8UEGqSjtc","colab_type":"text"},"source":["### configure datasets"]},{"cell_type":"code","metadata":{"id":"vNDCL8RoSnm_","colab_type":"code","colab":{}},"source":["synrod_train = SynRODMOD(synrod_path,\n","                item_extractor_fn=\"rotation\",\n","                item_extractor_param_values= synrod_param_values,\n","                synarid_path=\"/content/synARID_50k-split_sync_train1.txt\",\n","                augment=True)\n","synrod_test = SynRODMOD(synrod_path,\n","                item_extractor_fn=\"rotation\",\n","                item_extractor_param_values= synrod_param_values,\n","                 synarid_path=\"/content/synARID_50k-split_sync_test1.txt\")\n","rod = RODMOD(rod_path,\n","                item_extractor_fn=\"rotation\",\n","                item_extractor_param_values=rod_param_values,\n","                 rod_split_path=\"/content/rod-split_sync.txt\",\n","                augment=True)\n","rod_test = RODMOD(rod_path,\n","                item_extractor_fn=\"rotation\",\n","                item_extractor_param_values=rod_param_values,\n","                 rod_split_path=\"/content/rod-split_sync.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dkLZBxeVbGE","colab_type":"text"},"source":["# Train "]},{"cell_type":"code","metadata":{"id":"ItQ3BVkYpF1Z","colab_type":"code","colab":{}},"source":["# Save net\n","save_folder = \"/content/net_dumps\"\n","if not os.path.isdir(save_folder):\n","  !mkdir \"net_dumps\"\n","net_name = \"data_augmentation5_kaiming_init_20epochs\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpgMM6QQVamT","colab_type":"code","colab":{}},"source":["parameters_dict = { \"lr\" : [3e-4],\n","                    \"batch_size\":[64],\n","                   \"epochs\":[20],\n","                   \"weight_decay\":[5e-2],\n","                    \"step_size\": [7],\n","                   \"lambda\": [1.0]\n","                   }\n","\n","for grid in ParameterGrid(parameters_dict):\n","  net, train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = train_test_ours(synrod_train, \n","                                                                                           synrod_test, \n","                                                                                           rod, \n","                                                                                           rod_test, \n","                                                                                           grid, \n","                                                                                           save_folder, \n","                                                                                           net_name, \n","                                                                                           light_validation=False,\n","                                                                                           preexisting_net=None)\n","\n","  learning_curves(train_acc, train_loss, valid_acc, valid_loss, \"Data augmentation - Kaiming init\")\n","  learning_curves([], [], test_acc, test_loss, \"Test - Data augmentation - Kaiming init\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZ3C4xKcVtE6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRC3OGt-oe3K","colab_type":"text"},"source":["# Load net\n","If needed"]},{"cell_type":"code","metadata":{"id":"MHQhW3ruoglV","colab_type":"code","outputId":"4a6a49c3-d332-4ebd-915c-86b39ec8a334","executionInfo":{"status":"ok","timestamp":1592128916423,"user_tz":-120,"elapsed":14557,"user":{"displayName":"Matteo Bunino","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwZwD9gAYbPOfNJoxSUyCvD_aRYvQTPmt2RxWUZg=s64","userId":"14931582740568623850"}},"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["2ec919c1cb0c49a0a45c191921bd2fa0","91f832f09b3d47f4a6363709c334a3af","c249dab4acdd4403b0bc04193930d34a","99db823750d54931966bc47d0cf86460","57e81f877d4745ea981dbe5aceddbc23","c9de40259c7649d0bae5e54f423c5ee3","09d9b68dde52445ba4d54d8e57dbf2a3","1f14bdb40ce14c6e8047a8c568187ab7"]}},"source":["# Load net from folder\n","net_save_path = os.path.join(save_folder,  net_name + \".pth\"  )\n","\n","net2 = DNet(47, dim_pretext=5)\n","net2.load_state_dict(torch.load(net_save_path, map_location=\"cuda\"))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ec919c1cb0c49a0a45c191921bd2fa0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":16}]}]}